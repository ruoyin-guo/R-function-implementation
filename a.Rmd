---
title: "Implementation of Logistic Regression using R"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(readr)
library(tidyverse)
library(knitr)
```


Assume that we have observed two predictors, Xi1 and Xi2 and want to predict a binary outcome Yi (i.e. Yi = 0 or Yi = 1).
A logistic regression model assumes that the probability that Yi = 1 can be modelled using the following function of Xi1 = xi1 and Xi2 = xi2.
Pr(Yi = 1|Xi1 = xi1, Xi2 = xi2, θ1, θ2, θ3) = p(xi1, xi2) = 1/ (1 + exp(−xi1θ1 − xi2θ2 − θ3)).


Part a
Write a function to compute p(x1, x2) for n observations which takes as arguments:
i) A vector of three parameters θ = (θ1, θ2, θ3).
ii) Two predictor vectors, x1 = (x1,1, ..., xn,1) and x2 = (x1,2, ...xn,2)
and returns a length n vector corresponding to p(x11, p12), ...p(xn1, xn2) for the corresponding θ values

```{r a, echo=TRUE}

p_x1_x2 = function(theta,x1,x2){
  
  1/(1+exp(-x1*theta[1] - x2*theta[2]-theta[3]))
  
}


```


Part b
Write a function to compute L(θ1, θ2, θ3) cross-entropy loss:
L(θ1, θ2, θ3) = −Xni=1[yi × log(p(xi1, xi2)) + (1 − yi) × log(1 − p(xi1, xi2))]

```{r b,echo=TRUE}
 comp_L = function(theta,x1,x2,y){
   total = sum(y*log(p_x1_x2(theta,x1,x2)) + (1-y)*log(1-p_x1_x2(theta,x1,x2)))
   -1*total

 }
```


Part c
Fit a logistic regression classifier to the HTRU2 data.
Using optim and your loss function from part (b), find the values of theta[1], theta[2],
theta[3] that minimize the cross-entropy loss

```{r c, echo=TRUE}

HTRU2 = read_csv("HTRU2_varnames.csv")

result = with(HTRU2, 
              optim(par=c(0,0,0), f=comp_L,x1 = Mean_IP, 
               x2 = Mean_DMSNR,y=Class))

result$par
result$value
```   


Part d
Compute the minimized cross-entropy loss for each possible pair of predictors for the HTRU2 data.
Arrange the rows by the value of the loss to find create a table ordered from best pairs of predictors to worst pairs according to estimated loss.

```{r d,echo=TRUE}

var_combs<-combn(names(HTRU2[,-9]),2)
var_combs
Predictor1 = vector("character",28)
Predictor2 = vector("character",28)
Losses = vector("numeric",28)

# creates tibble to store x1,x2, and losses
output_tbl = tibble(Predictor1 = NULL,
                    Predictor2 = NULL,
                    Losses = NULL)

for (i in 1:28){
  x1 = HTRU2 %>% select(var_combs[1,i])
  x2 = HTRU2 %>% select(var_combs[2,i])
  result = with(HTRU2,optim(par=c(0,0,0), f=comp_L,
                 x1=x1, 
                 x2=x2,
               y=Class))
  Predictor1[i] = var_combs[1,i]
  Predictor2[i] = var_combs[2,i]
  Losses[i] = result$value
}

output_tbl = tibble(Predictor1,Predictor2,Losses)
output_tbl %>% arrange(Losses) %>% kable()

```   


Part e
Produce the same tibble as in part (d), only using the var_combs matrix above and map_dfr(.)
```{r e, echo=TRUE }

# convert matrix to df
var_combs_df = as.data.frame(var_combs)
var_combs_df


map_func = function(var_combs_df){
  x1 = HTRU2 %>% select(var_combs_df[1])
  
  x2 = HTRU2 %>% select(var_combs_df[2])
  result = with(HTRU2,optim(par=c(0,0,0), f=comp_L,
                 x1=x1, 
                 x2=x2,
               y=Class))
  Predictor1 = var_combs_df[1]
  Predictor2 = var_combs_df[2]
  Losses = result$value
  output = data.frame(Predictor1,Predictor2,Losses)
}

var_combs_df %>% map_dfr(map_func) %>% arrange(Losses) %>% kable()


```















































